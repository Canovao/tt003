{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j4kPzWVsUQZd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 1.0939018726348877\n",
            "1000 0.036327723413705826\n",
            "2000 0.03579600527882576\n",
            "3000 0.029503436759114265\n",
            "4000 0.025849001482129097\n",
            "5000 0.00021669748821295798\n",
            "6000 3.3463918953202665e-05\n",
            "7000 7.983807336131576e-06\n",
            "8000 2.3054415123624494e-06\n",
            "9000 6.914077630426618e-07\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2]) tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2])\n",
            "tensor(0.9667)\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "iris = sns.load_dataset(\"iris\")\n",
        "\n",
        "train=iris.sample(frac=0.8,random_state=42)\n",
        "test=iris.drop(train.index)\n",
        "\n",
        "# SEPARANDO AS COLUNAS\n",
        "x_train = train.loc[ : , [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]].values\n",
        "x_test  = test.loc[ : , [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]].values\n",
        "\n",
        "# TRANSFORMANDO EM NÚMERO\n",
        "train[\"species\"] = train.species.map({'setosa':0,'versicolor':1,'virginica':2})\n",
        "test[\"species\"]  = test.species.map({'setosa':0,'versicolor':1,'virginica':2})\n",
        "\n",
        "# TARGET TREINO E TESTE\n",
        "y_train = train.species.values\n",
        "y_test  = test.species.values\n",
        "\n",
        "# X\n",
        "x_train = torch.tensor(x_train, dtype = torch.float)\n",
        "x_test  = torch.tensor(x_test, dtype = torch.float)\n",
        "# Y\n",
        "y_train = torch.tensor(y_train, dtype = torch.long)\n",
        "y_test  = torch.tensor(y_test, dtype = torch.long)\n",
        "\n",
        "# NORMALIZANDO\n",
        "x_train = (x_train - x_train.min())/(x_train.max() - x_train.min())\n",
        "x_test  = (x_test  - x_test.min())/(x_test.max() - x_test.min())\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self,input_dim,output_dim):\n",
        "        super(NeuralNetwork,self).__init__()\n",
        "        self.input_layer    = nn.Linear(input_dim,128)\n",
        "        self.hidden_layer1  = nn.Linear(128,64)\n",
        "        self.output_layer   = nn.Linear(64,output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        out =  self.relu(self.input_layer(x))\n",
        "        out =  self.relu(self.hidden_layer1(out))\n",
        "        out =  self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "input_dim  = 4 ## Quatro características\n",
        "output_dim = 3 ## Três Espécies\n",
        "model = NeuralNetwork(input_dim,output_dim)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "epochs = 10000\n",
        "\n",
        "batch_size = 50\n",
        "batch_no = len(x_train) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(batch_no):\n",
        "\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "\n",
        "        x_var = x_train[start:end]\n",
        "        y_var = y_train[start:end]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output_train = model(x_var)\n",
        "\n",
        "        loss = loss_fn(output_train, y_var)\n",
        "\n",
        "        if (epoch % (epochs / 10) == 0 and i == 0) :\n",
        "            print(epoch, loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "y_pred = torch.argmax(model(x_test), dim = 1)\n",
        "\n",
        "print(y_pred, y_test)\n",
        "\n",
        "print((y_pred == y_test).sum()/len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tem nulos? False\n",
            "x_train, y_train\n",
            "tensor([[0.0025, 0.0421, 0.0316,  ..., 0.0418, 0.0421, 0.0260],\n",
            "        [0.0061, 0.0211, 0.0421,  ..., 0.0573, 0.0571, 0.0343],\n",
            "        [0.0042, 0.0421, 0.0526,  ..., 0.0501, 0.0499, 0.0311],\n",
            "        ...,\n",
            "        [0.0043, 0.0211, 0.0421,  ..., 0.0496, 0.0499, 0.0317],\n",
            "        [0.0115, 0.0421, 0.0316,  ..., 0.0697, 0.0705, 0.0426],\n",
            "        [0.0032, 0.0421, 0.0211,  ..., 0.0447, 0.0453, 0.0282]]) tensor([ 559., 2201., 1238.,  ..., 1169., 8364.,  526.])\n",
            "0 50842952.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m (epochs \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) :\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;28mprint\u001b[39m(epoch, loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m--> 110\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    114\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model(x_test), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:282\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    273\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    274\u001b[0m     (inputs,)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    279\u001b[0m )\n\u001b[0;32m    281\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 282\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:161\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    155\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         )\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    160\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 161\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "\n",
        "# NÃO HÁ NULOS, POR ISSO NÃO PRECISA TRATAR\n",
        "print('Tem nulos? ' + str(diamonds.isna().any().any()))\n",
        "\n",
        "\n",
        "# MAPS\n",
        "map_cut = {'Fair':0,'Good':1,'Very Good':2,'Premium':3,'Ideal':4}\n",
        "map_color = {'J':0,'I':1,'H':2,'G':3,'F':4,'E':5,'D':6}\n",
        "map_clarity = {'I1':0,'SI2':1,'SI1':2,'VS2':3,'VS1':4,'VVS2':5,'VVS1':6,'IF':7}\n",
        "\n",
        "\n",
        "train=diamonds.sample(frac=0.8,random_state=42)\n",
        "test=diamonds.drop(train.index)\n",
        "\n",
        "\n",
        "# TRANSFORMANDO EM NÚMERO\n",
        "train[\"cut\"] = train.cut.map(map_cut)\n",
        "test[\"cut\"]  = test.cut.map(map_cut)\n",
        "\n",
        "train[\"color\"] = train.color.map(map_color)\n",
        "test[\"color\"]  = test.color.map(map_color)\n",
        "\n",
        "train[\"clarity\"] = train.clarity.map(map_clarity)\n",
        "test[\"clarity\"]  = test.clarity.map(map_clarity)\n",
        "\n",
        "\n",
        "# SEPARANDO AS COLUNAS\n",
        "x_train = train.loc[ : ,  [\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"x\",\"y\",\"z\"]].values\n",
        "x_test  = test.loc[ : , [\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"x\",\"y\",\"z\"]].values\n",
        "\n",
        "\n",
        "# COLUNA QUE EU QUERO PREVER -> price -> dado um diamante, qual o preço dele?\n",
        "# TARGET TREINO E TESTE\n",
        "y_train = train.price.values\n",
        "y_test  = test.price.values\n",
        "\n",
        "\n",
        "# X\n",
        "x_train = torch.tensor(x_train, dtype = torch.float)\n",
        "x_test  = torch.tensor(x_test, dtype = torch.float)\n",
        "# Y\n",
        "y_train = torch.tensor(y_train, dtype = torch.float)\n",
        "y_test  = torch.tensor(y_test, dtype = torch.float)\n",
        "\n",
        "\n",
        "# NORMALIZANDO\n",
        "x_train = (x_train - x_train.min())/(x_train.max() - x_train.min())\n",
        "x_test  = (x_test  - x_train.min())/(x_train.max() - x_train.min())\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self,input_dim,output_dim):\n",
        "        super(NeuralNetwork,self).__init__()\n",
        "        self.input_layer    = nn.Linear(input_dim,128)\n",
        "        self.hidden_layer1  = nn.Linear(128,64)\n",
        "        self.output_layer   = nn.Linear(64,output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        out =  self.relu(self.input_layer(x))\n",
        "        out =  self.relu(self.hidden_layer1(out))\n",
        "        return  self.output_layer(out)\n",
        "\n",
        "\n",
        "input_dim  = 9 ## Quatro características\n",
        "output_dim = 1 ## PREÇO FINAL\n",
        "model = NeuralNetwork(input_dim,output_dim)\n",
        "\n",
        "# PROBLEMA NÃO É DE CLASSIFICAÇÃO, MAS DE REGRESSÃO\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "epochs = 10000\n",
        "\n",
        "# MINI-BATCH\n",
        "batch_size = 50\n",
        "batch_no = len(x_train) // batch_size\n",
        "\n",
        "\n",
        "\n",
        "print(\"x_train, y_train\")\n",
        "print(x_train, y_train)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(batch_no):\n",
        "\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "\n",
        "        x_var = x_train[start:end]\n",
        "        y_var = y_train[start:end]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output_train = model(x_var)\n",
        "\n",
        "        loss = loss_fn(output_train, y_var)\n",
        "\n",
        "        if (epoch % (epochs / 10) == 0 and i == 0) :\n",
        "            print(epoch, loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "y_pred = torch.argmax(model(x_test), dim = 1)\n",
        "\n",
        "print(y_pred, y_test)\n",
        "\n",
        "print((y_pred == y_test).sum()/len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tem algum nulo? False\n",
            "Erro Médio Absoluto: 3919.083740234375\n",
            "Porcentagem de Acerto: -0.006404370322371733\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# PEGANDO MENOS REGISTROS PORQUE SE NÃO MEU PC NÃO AGUENTA\n",
        "diamonds = sns.load_dataset(\"diamonds\")#.head(10000)\n",
        "\n",
        "\n",
        "# NÃO HÁ NULOS, POR ISSO NÃO PRECISA TRATAR\n",
        "print('Tem algum nulo? ' + str(diamonds.isna().any().any()))\n",
        "\n",
        "# MAPS\n",
        "map_cut = {'Fair':0,'Good':1,'Very Good':2,'Premium':3,'Ideal':4}\n",
        "map_color = {'J':0,'I':1,'H':2,'G':3,'F':4,'E':5,'D':6}\n",
        "map_clarity = {'I1':0,'SI2':1,'SI1':2,'VS2':3,'VS1':4,'VVS2':5,'VVS1':6,'IF':7}\n",
        "\n",
        "train=diamonds.sample(frac=0.8,random_state=42)\n",
        "test=diamonds.drop(train.index)\n",
        "\n",
        "# TRANSFORMANDO EM NÚMERO\n",
        "train[\"cut\"] = train.cut.map(map_cut)\n",
        "test[\"cut\"]  = test.cut.map(map_cut)\n",
        "\n",
        "train[\"color\"] = train.color.map(map_color)\n",
        "test[\"color\"]  = test.color.map(map_color)\n",
        "\n",
        "train[\"clarity\"] = train.clarity.map(map_clarity)\n",
        "test[\"clarity\"]  = test.clarity.map(map_clarity)\n",
        "\n",
        "# SEPARANDO AS COLUNAS\n",
        "x_train = train.loc[ : ,  [\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"x\",\"y\",\"z\"]].values\n",
        "x_test  = test.loc[ : , [\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"x\",\"y\",\"z\"]].values\n",
        "\n",
        "# COLUNA QUE EU QUERO PREVER -> price -> dado um diamante, qual o preço dele?\n",
        "# TARGET TREINO E TESTE\n",
        "y_train = train.price.values\n",
        "y_test  = test.price.values\n",
        "\n",
        "# X\n",
        "x_train = torch.tensor(x_train, dtype = torch.float)\n",
        "x_test  = torch.tensor(x_test, dtype = torch.float)\n",
        "# x_train = x_train.unsqueeze(1)\n",
        "# x_test = x_test.unsqueeze(1)\n",
        "# Y\n",
        "y_train = torch.tensor(y_train, dtype = torch.float)\n",
        "y_test  = torch.tensor(y_test, dtype = torch.float)\n",
        "# y_train = y_train.unsqueeze(1)\n",
        "# y_test = y_test.unsqueeze(1)\n",
        "\n",
        "# NORMALIZANDO\n",
        "x_train = (x_train - x_train.min())/(x_train.max() - x_train.min())\n",
        "x_test  = (x_test  - x_train.min())/(x_train.max() - x_train.min())\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, 128)\n",
        "        self.hidden_layer1 = nn.Linear(128, 64)\n",
        "        self.hidden_layer2 = nn.Linear(64, 32)  \n",
        "        self.output_layer = nn.Linear(32, output_dim)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.input_layer(x))\n",
        "        out = self.relu(self.hidden_layer1(out))\n",
        "        out = self.relu(self.hidden_layer2(out))\n",
        "        return self.output_layer(out)\n",
        "\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = 1 ## PREÇO FINAL\n",
        "model = NeuralNetwork(input_dim,output_dim)\n",
        "\n",
        "# PROBLEMA NÃO É DE CLASSIFICAÇÃO, MAS DE REGRESSÃO\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "epochs = 10000\n",
        "\n",
        "# MINI-BATCH\n",
        "batch_size = 10000000000\n",
        "batch_no = len(x_train) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(batch_no):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "\n",
        "        x_var = x_train[start:end]\n",
        "        y_var = y_train[start:end]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output_train = model(x_var).squeeze(-1) # SQUEEZE PRA FICAR NA MESMA DIMENSÃO\n",
        "\n",
        "        loss = loss_fn(output_train, y_var)\n",
        "\n",
        "        if (epoch % (epochs / 10) == 0 and i == 0) :\n",
        "            print(epoch, loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "y_pred = model(x_test)\n",
        "\n",
        "medio = torch.mean(torch.abs(y_pred.squeeze() - y_test))\n",
        "print('Erro Médio Absoluto:', medio.item())\n",
        "\n",
        "porcentagem = 100 - (medio.item() / torch.mean(torch.abs(y_test)).item() * 100)\n",
        "print('Porcentagem de Acerto:', porcentagem)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tem algum nulo? False\n",
            "Erro Médio Absoluto: 3919.474609375\n",
            "Porcentagem de Acerto: -0.016378491806918305\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# PEGANDO MENOS REGISTROS PORQUE SE NÃO MEU PC NÃO AGUENTA\n",
        "diamonds = sns.load_dataset(\"diamonds\")#.head(10000)\n",
        "\n",
        "print('Tem algum nulo? ' + str(diamonds.isna().any().any()))\n",
        "\n",
        "map_cut = {'Fair':0,'Good':1,'Very Good':2,'Premium':3,'Ideal':4}\n",
        "map_color = {'J':0,'I':1,'H':2,'G':3,'F':4,'E':5,'D':6}\n",
        "map_clarity = {'I1':0,'SI2':1,'SI1':2,'VS2':3,'VS1':4,'VVS2':5,'VVS1':6,'IF':7}\n",
        "\n",
        "train=diamonds.sample(frac=0.8,random_state=42)\n",
        "test=diamonds.drop(train.index)\n",
        "\n",
        "train[\"cut\"] = train.cut.map(map_cut)\n",
        "test[\"cut\"]  = test.cut.map(map_cut)\n",
        "\n",
        "train[\"color\"] = train.color.map(map_color)\n",
        "test[\"color\"]  = test.color.map(map_color)\n",
        "\n",
        "train[\"clarity\"] = train.clarity.map(map_clarity)\n",
        "test[\"clarity\"]  = test.clarity.map(map_clarity)\n",
        "\n",
        "x_train = train.loc[ : ,  [\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"x\",\"y\",\"z\"]].values\n",
        "x_test  = test.loc[ : , [\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"x\",\"y\",\"z\"]].values\n",
        "\n",
        "y_train = train.price.values\n",
        "y_test  = test.price.values\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype = torch.float)\n",
        "x_test  = torch.tensor(x_test, dtype = torch.float)\n",
        "\n",
        "y_train = torch.tensor(y_train, dtype = torch.float)\n",
        "y_test  = torch.tensor(y_test, dtype = torch.float)\n",
        "\n",
        "x_train = (x_train - x_train.min())/(x_train.max() - x_train.min())\n",
        "x_test  = (x_test  - x_train.min())/(x_train.max() - x_train.min())\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, 128)\n",
        "        self.hidden_layer1 = nn.Linear(128, 64)\n",
        "        self.output_layer = nn.Linear(64, 1)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.input_layer(x))\n",
        "        out = self.relu(self.hidden_layer1(out))\n",
        "        return self.output_layer(out)\n",
        "\n",
        "input_dim = x_train.shape[1]\n",
        "model = NeuralNetwork(input_dim)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "learning_rate = 0.001 \n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "epochs = 10000\n",
        "\n",
        "batch_size = 1000000000\n",
        "batch_no = len(x_train) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(batch_no):\n",
        "        start = i * batch_size\n",
        "        end = start + batch_size\n",
        "\n",
        "        x_var = x_train[start:end]\n",
        "        y_var = y_train[start:end]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output_train = model(x_var).squeeze(-1)\n",
        "\n",
        "        loss = loss_fn(output_train, y_var)\n",
        "\n",
        "        if (epoch % (epochs / 10) == 0 and i == 0) :\n",
        "            print(epoch, loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "y_pred = model(x_test).squeeze(-1)\n",
        "\n",
        "with open('y_pred.txt', 'w') as f:\n",
        "    f.write(str(y_pred[:100]))\n",
        "\n",
        "with open('y_test.txt', 'w') as f:\n",
        "    f.write(str(y_test[:100]))\n",
        "\n",
        "medio = torch.mean(torch.abs(y_pred.squeeze() - y_test))\n",
        "print('Erro Médio Absoluto:', medio.item())\n",
        "\n",
        "porcentagem = 100 - (medio.item() / torch.mean(torch.abs(y_test)).item() * 100)\n",
        "print('Porcentagem de Acerto:', porcentagem)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tem algum nulo? False\n",
            "Epoch [0/2000], Loss: 31439196.0000\n",
            "Epoch [100/2000], Loss: 28451764.0000\n",
            "Epoch [200/2000], Loss: 10601800.0000\n",
            "Epoch [300/2000], Loss: 3056218.5000\n",
            "Epoch [400/2000], Loss: 1945246.0000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[96], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X_train)\n\u001b[0;32m     74\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, y_train)\n\u001b[1;32m---> 75\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     77\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(loss)  \u001b[38;5;66;03m# Atualizando a taxa de aprendizado\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Carregando o conjunto de dados\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "print('Tem algum nulo? ' + str(diamonds.isna().any().any()))\n",
        "\n",
        "map_cut = {'Fair':0,'Good':1,'Very Good':2,'Premium':3,'Ideal':4}\n",
        "map_color = {'J':0,'I':1,'H':2,'G':3,'F':4,'E':5,'D':6}\n",
        "map_clarity = {'I1':0,'SI2':1,'SI1':2,'VS2':3,'VS1':4,'VVS2':5,'VVS1':6,'IF':7}\n",
        "\n",
        "# Transformando dados\n",
        "diamonds['cut'] = diamonds['cut'].map(map_cut)\n",
        "diamonds['color'] = diamonds['color'].map(map_color)\n",
        "diamonds['clarity'] = diamonds['clarity'].map(map_clarity)\n",
        "\n",
        "# Separando características e alvo\n",
        "X = diamonds[[\"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"x\", \"y\", \"z\"]].values\n",
        "y = diamonds['price'].values\n",
        "\n",
        "# Dividindo o conjunto em treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizando os dados\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convertendo para tensores\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Definindo a arquitetura da rede neural\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, 256)\n",
        "        self.hidden_layer1 = nn.Linear(256, 128)\n",
        "        self.dropout = nn.Dropout(0.2)  # Dropout de 20%\n",
        "        self.output_layer = nn.Linear(128, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.input_layer(x))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.hidden_layer1(out))\n",
        "        return self.output_layer(out)\n",
        "\n",
        "# Inicializando o modelo\n",
        "model = NeuralNetwork(input_dim=X_train.shape[1])\n",
        "\n",
        "# Definindo a função de perda e o otimizador\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Configurando o agendador de taxa de aprendizado\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "\n",
        "# Treinamento\n",
        "epochs = 2000\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train)\n",
        "    loss = loss_fn(output, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step(loss)  # Atualizando a taxa de aprendizado\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Early stopping\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        torch.save(model.state_dict(), 'best_model.pth')  # Salvando o melhor modelo\n",
        "\n",
        "# Testando o modelo\n",
        "model.eval()\n",
        "y_pred = model(X_test)\n",
        "\n",
        "# Erro médio absoluto\n",
        "mae = torch.mean(torch.abs(y_pred.squeeze() - y_test.squeeze()))\n",
        "print('Erro Médio Absoluto:', mae.item())\n",
        "\n",
        "# Porcentagem de acerto\n",
        "porcentagem = 100 - (mae.item() / torch.mean(torch.abs(y_test)).item() * 100)\n",
        "print('Porcentagem de Acerto:', porcentagem)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
